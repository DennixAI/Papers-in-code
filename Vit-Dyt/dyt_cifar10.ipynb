{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DyT CIFAR-10 Training\n",
        "\n",
        "Train a DynamicTanh Transformer (DyT) on CIFAR-10 with mixup and RandAugment. This notebook is self-contained and saves checkpoints, logs, and plots locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.cuda import amp\n",
        "\n",
        "from dyt import DyT\n",
        "from randomaug import RandAugment\n",
        "from utils import progress_bar\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters tuned to stay within ~8-10GB of VRAM.\n",
        "DATA_DIR = Path('./data')\n",
        "CHECKPOINT_DIR = Path('checkpoint')\n",
        "LOG_DIR = Path('log')\n",
        "PLOT_DIR = Path('plots')\n",
        "\n",
        "for directory in (CHECKPOINT_DIR, LOG_DIR, PLOT_DIR):\n",
        "    directory.mkdir(exist_ok=True)\n",
        "\n",
        "TOTAL_EPOCHS = 200\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "DIM = 384\n",
        "DEPTH = 6\n",
        "HEADS = 6\n",
        "MLP_DIM = 1536\n",
        "DROPOUT = 0.1\n",
        "EMB_DROPOUT = 0.1\n",
        "MIXUP_ALPHA = 0.2\n",
        "USE_MIXUP = True\n",
        "USE_RANDAUG = True\n",
        "USE_AMP = True\n",
        "NUM_WORKERS = 8\n",
        "\n",
        "CHECKPOINT_PATH = CHECKPOINT_DIR / 'dyt_cifar10_latest.pth'\n",
        "BEST_CHECKPOINT_PATH = CHECKPOINT_DIR / 'dyt_cifar10_best.pth'\n",
        "\n",
        "print('Configuration ready for CIFAR-10 DyT training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "train_transforms = [\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "]\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "])\n",
        "\n",
        "if USE_RANDAUG:\n",
        "    train_transforms.insert(0, RandAugment(2, 14))\n",
        "\n",
        "transform_train = transforms.Compose(train_transforms)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=str(DATA_DIR),\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train,\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=str(DATA_DIR),\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transform,\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "classes = trainset.classes\n",
        "print(f'Train batches: {len(trainloader)}, Test batches: {len(testloader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mixup_data(inputs, targets, alpha):\n",
        "    if not USE_MIXUP or alpha <= 0:\n",
        "        return inputs, targets, targets, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = inputs.size(0)\n",
        "    index = torch.randperm(batch_size, device=inputs.device)\n",
        "    mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
        "    targets_a = targets\n",
        "    targets_b = targets[index]\n",
        "    return mixed_inputs, targets_a, targets_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, predictions, targets_a, targets_b, lam):\n",
        "    if not USE_MIXUP:\n",
        "        return criterion(predictions, targets_a)\n",
        "    return lam * criterion(predictions, targets_a) + (1 - lam) * criterion(predictions, targets_b)\n",
        "\n",
        "def plot_metrics(train_losses, val_losses, val_accs):\n",
        "    if not train_losses:\n",
        "        return\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, val_accs, label='Val Acc (%)', color='tab:green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "\n",
        "    plot_path = PLOT_DIR / 'dyt_cifar10_training.png'\n",
        "    plt.suptitle('DyT on CIFAR-10')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show()\n",
        "    print(f'Saved training plot to {plot_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DyT(\n",
        "    image_size=IMAGE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dim=DIM,\n",
        "    depth=DEPTH,\n",
        "    heads=HEADS,\n",
        "    mlp_dim=MLP_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    emb_dropout=EMB_DROPOUT,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS)\n",
        "scaler = amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "start_epoch = 0\n",
        "best_acc = 0.0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "if CHECKPOINT_PATH.exists():\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    scaler.load_state_dict(checkpoint['scaler'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_acc = checkpoint.get('best_acc', 0.0)\n",
        "    train_losses = checkpoint.get('train_losses', [])\n",
        "    val_losses = checkpoint.get('val_losses', [])\n",
        "    val_accs = checkpoint.get('val_accs', [])\n",
        "    print(f'Resumed from epoch {start_epoch} with best acc {best_acc:.2f}%')\n",
        "else:\n",
        "    print('Starting fresh training run.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, best=False):\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'best_acc': best_acc,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "    }\n",
        "    path = BEST_CHECKPOINT_PATH if best else CHECKPOINT_PATH\n",
        "    torch.save(state, path)\n",
        "    label = 'best' if best else 'latest'\n",
        "    print(f'Checkpoint ({label}) saved to {path}')\n",
        "\n",
        "def log_epoch():\n",
        "    csv_path = LOG_DIR / 'dyt_cifar10_metrics.csv'\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['train_loss'] + train_losses)\n",
        "        writer.writerow(['val_loss'] + val_losses)\n",
        "        writer.writerow(['val_acc'] + val_accs)\n",
        "    print(f'Metrics updated at {csv_path}')\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        mixed_inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, MIXUP_ALPHA)\n",
        "\n",
        "        with amp.autocast(enabled=USE_AMP):\n",
        "            outputs = model(mixed_inputs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(\n",
        "            batch_idx,\n",
        "            len(trainloader),\n",
        "            'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n",
        "                running_loss / (batch_idx + 1),\n",
        "                100.0 * correct / total,\n",
        "                correct,\n",
        "                total,\n",
        "            ),\n",
        "        )\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(\n",
        "            batch_idx,\n",
        "            len(testloader),\n",
        "            'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n",
        "                running_loss / (batch_idx + 1),\n",
        "                100.0 * correct / total,\n",
        "                correct,\n",
        "                total,\n",
        "            ),\n",
        "        )\n",
        "    avg_loss = running_loss / len(testloader)\n",
        "    acc = 100.0 * correct / total\n",
        "    return avg_loss, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if start_epoch >= TOTAL_EPOCHS:\n",
        "    print('Training already completed for the configured number of epochs.')\n",
        "else:\n",
        "    for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{TOTAL_EPOCHS}')\n",
        "        train_loss = train_one_epoch(epoch)\n",
        "        val_loss, val_acc = evaluate(epoch)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        log_line = (\n",
        "            f\"{time.ctime()} | Epoch {epoch + 1}/{TOTAL_EPOCHS} | lr: {current_lr:.6f} | \"\n",
        "            f\"train loss: {train_loss:.4f} | val loss: {val_loss:.4f} | val acc: {val_acc:.2f}%\"\n",
        "        )\n",
        "\n",
        "        log_epoch()\n",
        "        with open(LOG_DIR / 'dyt_cifar10.log', 'a') as logfile:\n",
        "            logfile.write(log_line + '\\n')\n",
        "\n",
        "        save_checkpoint(epoch, best=False)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            save_checkpoint(epoch, best=True)\n",
        "\n",
        "        print(log_line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_metrics(train_losses, val_losses, val_accs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}