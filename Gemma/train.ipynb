{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Model Created. Params: 128.41M\n",
      "Starting Training Loop...\n",
      "Step 4 | Loss: 38.4384 | VRAM Used: 1.65 GB\n",
      "Step 8 | Loss: 33.9174 | VRAM Used: 1.65 GB\n",
      "Step 12 | Loss: 29.9557 | VRAM Used: 1.65 GB\n",
      "Step 16 | Loss: 26.4443 | VRAM Used: 1.65 GB\n",
      "Step 20 | Loss: 23.4977 | VRAM Used: 1.65 GB\n",
      "Step 24 | Loss: 21.1215 | VRAM Used: 1.65 GB\n",
      "Step 28 | Loss: 19.1322 | VRAM Used: 1.65 GB\n",
      "Step 32 | Loss: 17.3895 | VRAM Used: 1.65 GB\n",
      "Step 36 | Loss: 15.8175 | VRAM Used: 1.65 GB\n",
      "Step 40 | Loss: 14.4026 | VRAM Used: 1.65 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler \n",
    "import torch.utils.checkpoint\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (Optimized for 5GB VRAM)\n",
    "# ==========================================\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 32000      # Reduced from 256k to save VRAM\n",
    "    hidden_size: int = 768       # Decent size for reasoning\n",
    "    intermediate_size: int = 2048 # GeGLU expansion\n",
    "    num_hidden_layers: int = 16  # Deep enough for complex logic\n",
    "    num_attention_heads: int = 8\n",
    "    num_key_value_heads: int = 4 # GQA (Grouped Query Attention)\n",
    "    head_dim: int = 96\n",
    "    max_position_embeddings: int = 1024 # Context window\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0 # Theoretical Hack: Stability\n",
    "    final_logit_softcapping: float = 30.0\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOW-LEVEL MODULES\n",
    "# ==========================================\n",
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        # Calculate locally first to avoid attribute collision\n",
    "        inv_freq_tensor = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq_tensor, persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# ==========================================\n",
    "# 3. ATTENTION & MLP (The Brain)\n",
    "# ==========================================\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        \n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "\n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Soft-capping Hack\n",
    "        if self.config.attn_logit_softcapping is not None:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class GemmaMLP(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN MODEL ARCHITECTURE\n",
    "# ==========================================\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = GemmaMLP(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.self_attn(hidden_states, attention_mask=attention_mask)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(layer, x, attention_mask, use_reentrant=False)\n",
    "            else:\n",
    "                x = layer(x, attention_mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Tied Weights + Soft-capping on output\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping is not None:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP (Memory Optimized)\n",
    "# ==========================================\n",
    "def train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Running on: {device}\")\n",
    "    \n",
    "    config = GemmaZeroConfig()\n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    \n",
    "    # Enable memory saving\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    print(f\"Model Created. Params: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler('cuda') # For mixed precision\n",
    "\n",
    "    # Settings for 5GB VRAM\n",
    "    batch_size = 1\n",
    "    grad_accum_steps = 4\n",
    "    seq_len = 1024\n",
    "    \n",
    "    # Dummy Dataset\n",
    "    inputs = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)\n",
    "    labels = inputs.clone()\n",
    "\n",
    "    model.train()\n",
    "    print(\"Starting Training Loop...\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step in range(50):\n",
    "        \n",
    "        # Mixed Precision Context\n",
    "        with autocast('cuda', dtype=torch.float16):\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Shift for Causal LM loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
    "            loss = loss / grad_accum_steps # Normalize\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mem = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"Step {step+1} | Loss: {loss.item() * grad_accum_steps:.4f} | VRAM Used: {mem:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
