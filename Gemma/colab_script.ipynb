{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Hugging Face Repo: FusionCorp/gemma-zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "\n",
    "if not HF_TOKEN or not REPO_NAME:\n",
    "    raise ValueError(\"Error: HF_TOKEN or REPO_NAME not found in .env file.\")\n",
    "\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "try:\n",
    "    create_repo(repo_id=REPO_NAME, repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Connected to Hugging Face Repo: {REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\" Repo Connection Failed: {e}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 50257      # Must be 50257 for GPT2\n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        k, v = k.repeat_interleave(self.num_key_value_groups, dim=1), v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if self.config.attn_logit_softcapping:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        if attention_mask is not None: attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype), v)\n",
    "        return self.o_proj(attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1))\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # This line saves ~10GB of VRAM by re-calculating \n",
    "                # activations during the backward pass.\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, \n",
    "                    x, \n",
    "                    None,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tinystories_loader(batch_size=4, seq_len=2048):\n",
    "    print(\"Loading TinyStories...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # TinyStories is massive, so we stream it\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "    \n",
    "    def data_generator():\n",
    "        for item in dataset:\n",
    "            # Filter out very short stories\n",
    "            if len(item['text']) < 50: continue\n",
    "            \n",
    "            # Pack/Truncate\n",
    "            tokens = tokenizer.encode(item['text'], max_length=seq_len, truncation=True, padding=\"max_length\")\n",
    "            yield torch.tensor(tokens)\n",
    "            \n",
    "    gen = data_generator()\n",
    "    def get_batch():\n",
    "        batch = [next(gen) for _ in range(batch_size)]\n",
    "        return torch.stack(batch).cuda()\n",
    "    return get_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff45a5bd0248b2897a246af61b3692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training 400M Model | Brain Density: 24 Layers\n",
      "Effective Batch: 32 | Optimizer: 8-bit AdamW\n",
      "Step 32 | Loss: 26.7978 | Peak VRAM: 8.04 GB\n",
      "Step 64 | Loss: 21.8812 | Peak VRAM: 8.04 GB\n",
      "Step 96 | Loss: 19.9212 | Peak VRAM: 8.04 GB\n",
      "Step 128 | Loss: 17.2931 | Peak VRAM: 8.04 GB\n",
      "Step 160 | Loss: 14.2393 | Peak VRAM: 8.04 GB\n",
      "Step 192 | Loss: 16.1883 | Peak VRAM: 8.04 GB\n",
      "â˜ï¸ Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2799ba5fe34d497198672b950e365812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c5a9e7ae6849dd95ce4960c719c3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5db4c41176948468f452c101bec8efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 200\n",
      "Step 224 | Loss: 14.0803 | Peak VRAM: 8.04 GB\n",
      "Step 256 | Loss: 14.3976 | Peak VRAM: 8.04 GB\n",
      "Step 288 | Loss: 13.5689 | Peak VRAM: 8.04 GB\n",
      "Step 320 | Loss: 12.3272 | Peak VRAM: 8.04 GB\n",
      "Step 352 | Loss: 15.1846 | Peak VRAM: 8.04 GB\n",
      "Step 384 | Loss: 15.2465 | Peak VRAM: 8.04 GB\n",
      "â˜ï¸ Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e98ddc05a324eea8dee00370d0d7910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90307535c29d4c43a7edc0a4e61e90e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d308646b5f4e8898b2f93ac6f65d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 400\n",
      "Step 416 | Loss: 13.2133 | Peak VRAM: 8.04 GB\n",
      "Step 448 | Loss: 11.7567 | Peak VRAM: 8.04 GB\n",
      "Step 480 | Loss: 13.4908 | Peak VRAM: 8.04 GB\n",
      "Step 512 | Loss: 12.5683 | Peak VRAM: 8.04 GB\n",
      "Step 544 | Loss: 12.0133 | Peak VRAM: 8.04 GB\n",
      "Step 576 | Loss: 14.6466 | Peak VRAM: 8.04 GB\n",
      "â˜ï¸ Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17608d4d77d44bb833b7ec39075ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd121f340a7427c9ce385be8c1e23dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caeb72d13a24925b5424010a6f236dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 600\n",
      "Step 608 | Loss: 12.0771 | Peak VRAM: 8.04 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1682137808.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1682137808.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q bitsandbytes \n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup # Add this import\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    config = GemmaZeroConfig() \n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable() \n",
    "\n",
    "    \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    BATCH_SIZE = 2      \n",
    "    ACCUM_STEPS = 16    \n",
    "    SEQ_LEN = 2048\n",
    "    TOTAL_STEPS = 5000\n",
    "    \n",
    "     ADD SCHEDULER (Warmup for 100 steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=100, \n",
    "        num_training_steps=TOTAL_STEPS\n",
    "    )\n",
    "    \n",
    "    get_batch = get_tinystories_loader(BATCH_SIZE, SEQ_LEN)\n",
    "    pad_token_id = 50256\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    print(f\"ðŸš€ Training 400M Model | Stability Mode: ON\")\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "        inputs = get_batch()\n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        # SOTA Masking\n",
    "        labels[labels == pad_token_id] = -100\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): \n",
    "            logits = model(inputs)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size), \n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100 \n",
    "            )\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            # 3. ADD GRADIENT CLIPPING (The Stabilizer)\n",
    "            # This prevents the loss from exploding to 12.0+\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Monitoring\n",
    "            if (step + 1) % (ACCUM_STEPS * 2) == 0:\n",
    "                mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "                print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f} | Peak VRAM: {mem:.2f} GB\")\n",
    "        \n",
    "        # Upload Logic (Same as before)\n",
    "        if (step + 1) % 200 == 0:\n",
    "            # ... (Your existing upload code here)\n",
    "            pass \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
