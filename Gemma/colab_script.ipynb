{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 32000      \n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        k, v = k.repeat_interleave(self.num_key_value_groups, dim=1), v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if self.config.attn_logit_softcapping:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        if attention_mask is not None: attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype), v)\n",
    "        return self.o_proj(attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1))\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(layer, x, None, use_reentrant=False)\n",
    "            else: x = layer(x)\n",
    "        logits = torch.matmul(self.norm(x), self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_fitness_savant_dataloader(batch_size=4, seq_len=2048):\n",
    "    print(\"Loading Fitness, Diet & Rehab datasets...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    try:\n",
    "        ds_gym = load_dataset(\"onurSakar/GYM-Exercise\", split=\"train\")\n",
    "        def fmt_gym(x):\n",
    "            return {\"text\": f\"<|user|>\\nHow do I do the {x['Title']} exercise?\\n<|model|>\\n{x['Desc']}<|endoftext|>\"}\n",
    "        ds_gym = ds_gym.map(fmt_gym, remove_columns=ds_gym.column_names)\n",
    "        print(f\"   - Loaded Gym Exercises: {len(ds_gym)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Gym Exercises: {e}\")\n",
    "        ds_gym = None\n",
    "\n",
    "    try:\n",
    "        ds_diet = load_dataset(\"issai/LLM_for_Dietary_Recommendation_System\", split=\"train\")\n",
    "        def fmt_diet(x):\n",
    "            return {\"text\": f\"<|user|>\\nCreate a diet plan for this profile:\\n{x['Profile']}\\n<|model|>\\n{x['Recommendation']}<|endoftext|>\"}\n",
    "        ds_diet = ds_diet.map(fmt_diet, remove_columns=ds_diet.column_names)\n",
    "        print(f\"   - Loaded Diet Plans: {len(ds_diet)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Diet Plans: {e}\")\n",
    "        ds_diet = None\n",
    "\n",
    "    try:\n",
    "        ds_qa = load_dataset(\"kishkath/fitness-qa\", split=\"train\")\n",
    "        def fmt_qa(x):\n",
    "            q = x.get('instruction') or x.get('input') or x.get('question')\n",
    "            a = x.get('output') or x.get('answer')\n",
    "            return {\"text\": f\"<|user|>\\n{q}\\n<|model|>\\n{a}<|endoftext|>\"}\n",
    "        ds_qa = ds_qa.map(fmt_qa, remove_columns=ds_qa.column_names)\n",
    "        print(f\"   - Loaded Fitness QA: {len(ds_qa)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Fitness QA: {e}\")\n",
    "        ds_qa = None\n",
    "\n",
    "    try:\n",
    "        ds_chat = load_dataset(\"chibbss/fitness-chat-prompt-completion-dataset\", split=\"train\")\n",
    "        def fmt_chat(x): \n",
    "            return {\"text\": f\"<|user|>\\n{x['instruction']}\\n<|model|>\\n{x['output']}<|endoftext|>\"}\n",
    "        ds_chat = ds_chat.map(fmt_chat, remove_columns=ds_chat.column_names)\n",
    "        print(f\"   - Loaded Chat Data: {len(ds_chat)}\")\n",
    "    except Exception as e:\n",
    "        ds_chat = None\n",
    "\n",
    "    valid_datasets = [d for d in [ds_gym, ds_diet, ds_qa, ds_chat] if d is not None]\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        print(\"CRITICAL: No datasets loaded. Fallback to TinyStories.\")\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    else:\n",
    "        dataset = concatenate_datasets(valid_datasets).shuffle(seed=42)\n",
    "    \n",
    "    print(f\"Combined 'Savant' Dataset Size: {len(dataset)} examples\")\n",
    "\n",
    "    def data_generator():\n",
    "        while True:\n",
    "            for item in dataset:\n",
    "                try:\n",
    "                    tokens = tokenizer.encode(item['text'], max_length=seq_len, truncation=True, padding=\"max_length\")\n",
    "                    yield torch.tensor(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    gen = data_generator()\n",
    "    def get_batch():\n",
    "        batch = [next(gen) for _ in range(batch_size)]\n",
    "        return torch.stack(batch).cuda()\n",
    "    \n",
    "    return get_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3561098549.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler('cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 397.72M\n",
      "Loading Fitness, Diet & Rehab datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f780b686d5614fcb81a9d1844456a598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a710f19a245a44db8faf284ae972a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24712dd5d5b649219e9c065a24b00ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42c195c795844ff896740e34904d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c4c683c48f4a3abf6d59898b8b119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c99937fea4749d0939198e237fd0603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f735b965e4994b3f4540035dd0000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fa5bbdc5004688b9a47eee9e23e1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f750c5c96ffc48cba20813638b71056c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1660 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Gym Exercises: 'Title'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1bd515714e44bda1a2d03ebd3363de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960c606d9b054c59bb36cb1cd972fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cases_results.zip:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3104e6704d429f9eeba1209398fb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cases_results_1.zip:   0%|          | 0.00/158k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1623204ceb4286b6cff14b87cd5d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cases_results_1_tr.zip:   0%|          | 0.00/191k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2660bddcfc7f47959536b68b7e331076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cases_results_2.zip:   0%|          | 0.00/159k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbca80f15e44fd4a23933f32f6d4087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cases_results_2_tr.zip:   0%|          | 0.00/182k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc387de5ab14e73ba07a1cc8b408abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa0da0579ca44c185a918a267eaaccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Diet Plans: 'Profile'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb99b5e1b0e49afa23f4823f8463dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b844e6f2b3347ce91c00a5b93471b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpo_train.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1f4ec4e73d4edaa757074654e7d2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lora_train.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650b8e9345e84302b7e248056a775c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpo_val.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dded70b93b2c4e0dbabda1910a3dcd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lora_val.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412e16820ac42a796758afa7d68bbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading Fitness QA: An error occurred while generating the dataset\n",
      "\n",
      "All the data files must have the same columns, but at some point there are 1 new columns ({'answer'}) and 2 missing columns ({'chosen', 'rejected'}).\n",
      "\n",
      "This happened while the json dataset builder was generating data using\n",
      "\n",
      "hf://datasets/kishkath/fitness-qa/lora_train.json (at revision a37927ed836e75e01fe05d75c749f0ea90dcc94e)\n",
      "\n",
      "Please either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26aaf0611c1c44d191c5535b0da00f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ness-chat-prompt-completion-dataset.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e055777bcab44ea4a64f5eddc46de8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0053a00fe0f4317bdfc96666448974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Loaded Chat Data: 245\n",
      "Combined 'Savant' Dataset Size: 245 examples\n",
      "Starting Training (Fitness Savant Mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3561098549.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast('cuda', dtype=torch.bfloat16):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_autocast_enabled(): argument 'enabled' (position 2) must be bool, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3561098549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-3561098549.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# TODO: discuss a unified TorchScript-friendly API for autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_fastdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_autocast_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_increment_nesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_autocast_enabled(): argument 'enabled' (position 2) must be bool, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Running on: {device}\")\n",
    "    \n",
    "    config = GemmaZeroConfig()\n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable() \n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    BATCH_SIZE = 4 \n",
    "    ACCUM_STEPS = 8 \n",
    "    SEQ_LEN = 2048 \n",
    "    TOTAL_STEPS = 500 \n",
    "\n",
    "    get_batch = get_fitness_savant_dataloader(BATCH_SIZE, SEQ_LEN)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Starting Training (Fitness Savant Mode)...\")\n",
    "    \n",
    "    for step in range(TOTAL_STEPS):\n",
    "        inputs = get_batch()\n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        with autocast('cuda', dtype=torch.bfloat16): \n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(logits.view(-1, config.vocab_size), labels.view(-1))\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mem = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f} | VRAM: {mem:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
