{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 32000      \n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        k, v = k.repeat_interleave(self.num_key_value_groups, dim=1), v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if self.config.attn_logit_softcapping:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        if attention_mask is not None: attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype), v)\n",
    "        return self.o_proj(attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1))\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(layer, x, None, use_reentrant=False)\n",
    "            else: x = layer(x)\n",
    "        logits = torch.matmul(self.norm(x), self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness_savant_dataloader(batch_size=4, seq_len=2048):\n",
    "    print(\"Loading datasets...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def get_col(row, candidates, default=\"\"):\n",
    "        for col in candidates:\n",
    "            if col in row and row[col]:\n",
    "                return str(row[col])\n",
    "        return default\n",
    "\n",
    "    try:\n",
    "        ds_gym = load_dataset(\"onurSakar/GYM-Exercise\", split=\"train\")\n",
    "        def fmt_gym(x):\n",
    "            title = get_col(x, ['Title', 'title', 'Exercise', 'Exercise Name', 'instruction'])\n",
    "            desc = get_col(x, ['Desc', 'desc', 'Description', 'context', 'output'])\n",
    "            return {\"text\": f\"<|user|>\\nHow do I do the {title} exercise?\\n<|model|>\\n{desc}<|endoftext|>\"}\n",
    "        ds_gym = ds_gym.map(fmt_gym, remove_columns=ds_gym.column_names)\n",
    "        print(f\"Loaded Gym Exercises: {len(ds_gym)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Gym Exercises: {e}\")\n",
    "        ds_gym = None\n",
    "\n",
    "    try:\n",
    "        ds_diet = load_dataset(\"issai/LLM_for_Dietary_Recommendation_System\", split=\"train\")\n",
    "        def fmt_diet(x):\n",
    "            profile = get_col(x, ['Profile', 'profile', 'input'])\n",
    "            rec = get_col(x, ['Recommendation', 'recommendation', 'output'])\n",
    "            return {\"text\": f\"<|user|>\\nCreate a diet plan for this profile:\\n{profile}\\n<|model|>\\n{rec}<|endoftext|>\"}\n",
    "        ds_diet = ds_diet.map(fmt_diet, remove_columns=ds_diet.column_names)\n",
    "        print(f\"Loaded Diet Plans: {len(ds_diet)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Diet Plans: {e}\")\n",
    "        ds_diet = None\n",
    "\n",
    "    try:\n",
    "        ds_qa = load_dataset(\"kishkath/fitness-qa\", split=\"train\")\n",
    "        def fmt_qa(x):\n",
    "            q = get_col(x, ['instruction', 'question', 'input', 'Question'])\n",
    "            a = get_col(x, ['output', 'answer', 'Answer'])\n",
    "            return {\"text\": f\"<|user|>\\n{q}\\n<|model|>\\n{a}<|endoftext|>\"}\n",
    "        ds_qa = ds_qa.map(fmt_qa, remove_columns=ds_qa.column_names)\n",
    "        print(f\"Loaded Fitness QA: {len(ds_qa)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Fitness QA: {e}\")\n",
    "        ds_qa = None\n",
    "\n",
    "    try:\n",
    "        ds_chat = load_dataset(\"chibbss/fitness-chat-prompt-completion-dataset\", split=\"train\")\n",
    "        def fmt_chat(x): \n",
    "            q = get_col(x, ['instruction', 'prompt'])\n",
    "            a = get_col(x, ['output', 'completion'])\n",
    "            return {\"text\": f\"<|user|>\\n{q}\\n<|model|>\\n{a}<|endoftext|>\"}\n",
    "        ds_chat = ds_chat.map(fmt_chat, remove_columns=ds_chat.column_names)\n",
    "        print(f\"Loaded Chat Data: {len(ds_chat)}\")\n",
    "    except Exception as e:\n",
    "        ds_chat = None\n",
    "\n",
    "    valid_datasets = [d for d in [ds_gym, ds_diet, ds_qa, ds_chat] if d is not None]\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        print(\"Dataset load failure. Using TinyStories fallback.\")\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "    else:\n",
    "        dataset = concatenate_datasets(valid_datasets).shuffle(seed=42)\n",
    "    \n",
    "    print(f\"Total Combined Examples: {len(dataset)}\")\n",
    "\n",
    "    def data_generator():\n",
    "        while True:\n",
    "            for item in dataset:\n",
    "                try:\n",
    "                    tokens = tokenizer.encode(item['text'], max_length=seq_len, truncation=True, padding=\"max_length\")\n",
    "                    yield torch.tensor(tokens)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    gen = data_generator()\n",
    "    def get_batch():\n",
    "        batch = [next(gen) for _ in range(batch_size)]\n",
    "        return torch.stack(batch).cuda()\n",
    "    \n",
    "    return get_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Model Parameters: 397.72M\n",
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Running on: {device}\")\n",
    "    \n",
    "    config = GemmaZeroConfig()\n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable() \n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    #scaler = GradScaler('cuda')\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    BATCH_SIZE = 4 \n",
    "    ACCUM_STEPS = 8 \n",
    "    SEQ_LEN = 2048 \n",
    "    TOTAL_STEPS = 500 \n",
    "\n",
    "    get_batch = get_fitness_savant_dataloader(BATCH_SIZE, SEQ_LEN)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Starting Training (Fitness Savant Mode)...\")\n",
    "    \n",
    "    for step in range(TOTAL_STEPS):\n",
    "        inputs = get_batch()\n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(logits.view(-1, config.vocab_size), labels.view(-1))\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mem = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f} | VRAM: {mem:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
