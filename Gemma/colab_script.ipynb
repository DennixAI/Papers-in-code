{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Hugging Face Repo: FusionCorp/gemma-zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#load_dotenv()\n",
    "HF_TOKEN = \"\"\n",
    "REPO_NAME = \"\"\n",
    "\n",
    "if not HF_TOKEN or not REPO_NAME:\n",
    "    raise ValueError(\"Error: HF_TOKEN or REPO_NAME not found in .env file.\")\n",
    "\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "try:\n",
    "    create_repo(repo_id=REPO_NAME, repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Connected to Hugging Face Repo: {REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\" Repo Connection Failed: {e}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 50257      # Must be 50257 for GPT2\n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        k, v = k.repeat_interleave(self.num_key_value_groups, dim=1), v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if self.config.attn_logit_softcapping:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        if attention_mask is not None: attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype), v)\n",
    "        return self.o_proj(attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1))\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # This line saves ~10GB of VRAM by re-calculating \n",
    "                # activations during the backward pass.\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, \n",
    "                    x, \n",
    "                    None,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tinystories_loader(batch_size=4, seq_len=2048):\n",
    "    print(\"Loading TinyStories...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # TinyStories is massive, so we stream it\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "    \n",
    "    def data_generator():\n",
    "        for item in dataset:\n",
    "            # Filter out very short stories\n",
    "            if len(item['text']) < 50: continue\n",
    "            \n",
    "            # Pack/Truncate\n",
    "            tokens = tokenizer.encode(item['text'], max_length=seq_len, truncation=True, padding=\"max_length\")\n",
    "            yield torch.tensor(tokens)\n",
    "            \n",
    "    gen = data_generator()\n",
    "    def get_batch():\n",
    "        batch = [next(gen) for _ in range(batch_size)]\n",
    "        return torch.stack(batch).cuda()\n",
    "    return get_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff45a5bd0248b2897a246af61b3692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training 400M Model | Brain Density: 24 Layers\n",
      "Effective Batch: 32 | Optimizer: 8-bit AdamW\n",
      "Step 32 | Loss: 26.7978 | Peak VRAM: 8.04 GB\n",
      "Step 64 | Loss: 21.8812 | Peak VRAM: 8.04 GB\n",
      "Step 96 | Loss: 19.9212 | Peak VRAM: 8.04 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q bitsandbytes \n",
    "\n",
    "import bitsandbytes as bnb # Import the 8-bit optimizer\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\"\n",
    "    config = GemmaZeroConfig() \n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable() \n",
    "\n",
    "    # 8-Bit Optimizer (Saves 2.4GB VRAM)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    # Hyperparameters for 16GB\n",
    "    BATCH_SIZE = 2      # Lowered to 2 to prevent OOM\n",
    "    ACCUM_STEPS = 16    # Effective Batch Size = 32 (2 * 16)\n",
    "    SEQ_LEN = 2048\n",
    "    TOTAL_STEPS = 5000\n",
    "    \n",
    "    get_batch = get_tinystories_loader(BATCH_SIZE, SEQ_LEN)\n",
    "    pad_token_id = 50256\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True) # Saves extra memory\n",
    "    \n",
    "    print(f\" Training 400M Model | Brain Density: {config.num_hidden_layers} Layers\")\n",
    "    print(f\"Effective Batch: {BATCH_SIZE * ACCUM_STEPS} | Optimizer: 8-bit AdamW\")\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "        inputs = get_batch()\n",
    "        \n",
    "        # Safety Check\n",
    "        if inputs.max() >= config.vocab_size:\n",
    "            print(f\"Error: Token {inputs.max()} exceeds vocab {config.vocab_size}\")\n",
    "            break\n",
    "            \n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16): \n",
    "            logits = model(inputs)\n",
    "            \n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # --- SOTA FIX: MASK PADDING ---\n",
    "            # Set all padding tokens in the label to -100 so loss ignores them\n",
    "            shift_labels[shift_labels == pad_token_id] = -100\n",
    "            \n",
    "            # Loss will now IGNORE padding tokens!\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, config.vocab_size), \n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100 \n",
    "            )\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Monitoring\n",
    "            if (step + 1) % (ACCUM_STEPS * 2) == 0:\n",
    "                mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "                print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f} | Peak VRAM: {mem:.2f} GB\")\n",
    "        \n",
    "        if (step + 1) % 200 == 0:\n",
    "                print(f\"☁️ Uploading Checkpoint to {REPO_NAME}...\")\n",
    "                \n",
    "                # 1. Save weights locally first\n",
    "                local_file = \"pytorch_model.bin\"\n",
    "                torch.save(model.state_dict(), local_file)\n",
    "                \n",
    "                # 2. Push to Hugging Face\n",
    "                try:\n",
    "                    api.upload_file(\n",
    "                        path_or_fileobj=local_file,\n",
    "                        path_in_repo=\"pytorch_model.bin\", # Overwrites the file in the repo\n",
    "                        repo_id=REPO_NAME,\n",
    "                        repo_type=\"model\"\n",
    "                    )\n",
    "                    print(f\"Upload Success at Step {step+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Upload Failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
