{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.2)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.10.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Connected to Hugging Face Repo: FusionCorp/GemmaZero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "!pip install bitsandbytes\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "!rm -f full_checkpoint.pth\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#load_dotenv()\n",
    "HF_TOKEN = \"\"\n",
    "REPO_NAME = \"FusionCorp/GemmaZero\"\n",
    "\n",
    "if not HF_TOKEN or not REPO_NAME:\n",
    "    raise ValueError(\"Error: HF_TOKEN or REPO_NAME not found in .env file.\")\n",
    "\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "try:\n",
    "    create_repo(repo_id=REPO_NAME, repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Connected to Hugging Face Repo: {REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\" Repo Connection Failed: {e}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 50257      # Must be 50257 for GPT2\n",
    "    hidden_size: int = 768    \n",
    "    intermediate_size: int = 3072\n",
    "    num_hidden_layers: int = 6\n",
    "    num_attention_heads: int = 6\n",
    "    num_key_value_heads: int = 2\n",
    "    head_dim: int = 128\n",
    "    max_position_embeddings: int = 1024\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "431b17ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n",
      "Tesla T4, 15360 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "    print(f\"Running on TPU: {os.environ['COLAB_TPU_ADDR']}\")\n",
    "elif os.system(\"nvidia-smi > /dev/null 2>&1\") == 0:\n",
    "    print(\"Running on GPU\")\n",
    "    !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "        # GEMMA 3: QK-Norm (RMSNorm on Queries and Keys)\n",
    "        # This stabilizes training and allows us to use Flash Attention\n",
    "        self.q_norm = GemmaRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
    "        self.k_norm = GemmaRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        \n",
    "        # 1. Projections\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        # 2. QK Norm (Gemma 3 feature)\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        # 3. RoPE\n",
    "        # Transpose for RoPE: (bsz, heads, seq, dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. GQA Expansion\n",
    "        # (Expand K and V to match Q heads for calculation)\n",
    "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "\n",
    "        # 5. Flash Attention (Much faster, less memory)\n",
    "        # We drop manual soft-capping here to enable Flash Attention. \n",
    "        # QK-Norm handles the stability role of soft-capping.\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v, \n",
    "            attn_mask=None, # Flash attn handles causal mask internally if is_causal=True\n",
    "            dropout_p=0.0, \n",
    "            is_causal=True\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # This line saves ~10GB of VRAM by re-calculating \n",
    "                # activations during the backward pass.\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, \n",
    "                    x, \n",
    "                    None,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class TinyStoriesDataset(IterableDataset):\n",
    "    def __init__(self, seq_len=2048):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        for item in iterator:\n",
    "            if len(item['text']) < 50: continue\n",
    "            \n",
    "            # Simple truncation/padding\n",
    "            tokens = self.tokenizer(\n",
    "                item['text'], \n",
    "                max_length=self.seq_len, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            yield tokens.input_ids.squeeze(0)\n",
    "\n",
    "def get_tinystories_loader(batch_size=4, seq_len=2048):\n",
    "    ds = TinyStoriesDataset(seq_len=seq_len)\n",
    "    # num_workers=2 runs tokenization in parallel background processes\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-209648633.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-209648633.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Run 10 batches for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                         \u001b[0mv_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                         \u001b[0mv_lab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mv_lab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_lab\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    config = GemmaZeroConfig()\n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    FULL_CHECKPOINT_NAME = \"full_checkpoint.pth\"\n",
    "    LIGHT_WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "    LOSS_FILE = \"loss.txt\"\n",
    "\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=6e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    BATCH_SIZE = 8    \n",
    "    ACCUM_STEPS = 4     \n",
    "    SEQ_LEN = 1024       \n",
    "    TOTAL_STEPS = 20000  \n",
    "    \n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, 100, TOTAL_STEPS // ACCUM_STEPS)\n",
    "    \n",
    "    start_step = 0\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_ppl_history = []\n",
    "    val_steps = []\n",
    "    \n",
    "    if not os.path.exists(FULL_CHECKPOINT_NAME) and HF_TOKEN and HF_TOKEN != \"hf_...\":\n",
    "        try:\n",
    "            checkpoint_path = hf_hub_download(repo_id=REPO_NAME, filename=\"latest_full_checkpoint.pth\", token=HF_TOKEN)\n",
    "            import shutil\n",
    "            shutil.copy(checkpoint_path, FULL_CHECKPOINT_NAME)\n",
    "        except: pass\n",
    "\n",
    "    if os.path.exists(FULL_CHECKPOINT_NAME):\n",
    "        checkpoint = torch.load(FULL_CHECKPOINT_NAME, map_location=device)\n",
    "        \n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "        \n",
    "        model.load_state_dict(new_state_dict)\n",
    "        \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_step = checkpoint['step'] + 1\n",
    "        loss_history = checkpoint.get('loss_history', [])\n",
    "        val_loss_history = checkpoint.get('val_loss_history', [])\n",
    "        val_ppl_history = checkpoint.get('val_ppl_history', [])\n",
    "        val_steps = checkpoint.get('val_steps', [])\n",
    "        print(f\"‚úÖ Resumed at Step {start_step}\")\n",
    "\n",
    "    # Temporary fix\n",
    "    #try: model = torch.compile(model)\n",
    "    #except: pass\n",
    "\n",
    "    train_loader = DataLoader(TinyStoriesDataset(SEQ_LEN), batch_size=BATCH_SIZE)\n",
    "    val_ds = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", streaming=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    data_iter = iter(train_loader)\n",
    "    pad_token_id = 50256\n",
    "    model.train()\n",
    "\n",
    "    for step in range(start_step, TOTAL_STEPS):\n",
    "        try: inputs = next(data_iter).to(device)\n",
    "        except StopIteration: data_iter = iter(train_loader); inputs = next(data_iter).to(device)\n",
    "        \n",
    "        labels = inputs.clone()\n",
    "        labels[labels == pad_token_id] = -100\n",
    "\n",
    "        # --- CRITICAL FIX: CHANGED TO float16 (T4 Support) ---\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(logits[..., :-1, :].contiguous().view(-1, config.vocab_size), labels[..., 1:].contiguous().view(-1), ignore_index=-100)\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        actual_loss = loss.item() * ACCUM_STEPS\n",
    "        loss_history.append(actual_loss)\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Validation and Plotting\n",
    "            if (step + 1) % 500 == 0:\n",
    "                model.eval()\n",
    "                v_losses = []\n",
    "                v_iter = iter(val_loader)\n",
    "                for _ in range(10): # Run 10 batches for validation\n",
    "                    try: \n",
    "                        v_in = next(v_iter).to(device)\n",
    "                        v_lab = v_in.clone()\n",
    "                        v_lab[v_lab == pad_token_id] = -100\n",
    "                        with torch.no_grad(), autocast(device_type='cuda', dtype=torch.float16): # switch to float16\n",
    "                            v_logits = model(v_in)\n",
    "                            v_loss = F.cross_entropy(v_logits[..., :-1, :].contiguous().view(-1, config.vocab_size), v_lab[..., 1:].contiguous().view(-1), ignore_index=-100)\n",
    "                            v_losses.append(v_loss.item())\n",
    "                    except StopIteration: break\n",
    "                \n",
    "                avg_v_loss = sum(v_losses)/len(v_losses)\n",
    "                val_loss_history.append(avg_v_loss)\n",
    "                val_ppl_history.append(math.exp(avg_v_loss))\n",
    "                val_steps.append(step + 1)\n",
    "                model.train()\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "                ax1.plot(loss_history, color='blue', alpha=0.2, label='Train Loss')\n",
    "                if len(loss_history) > 50:\n",
    "                    ma = [sum(loss_history[i-50:i])/50 for i in range(50, len(loss_history))]\n",
    "                    ax1.plot(range(50, len(loss_history)), ma, color='blue', label='Train Trend')\n",
    "                ax1.plot(val_steps, val_loss_history, 'o-', color='red', label='Val Loss')\n",
    "                ax1.set_title(f\"Loss | Step {step+1}: {actual_loss:.4f}\")\n",
    "                ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                ax2.plot(val_steps, val_ppl_history, 'o-', color='green', label='Val Perplexity')\n",
    "                ax2.set_title(f\"Perplexity: {val_ppl_history[-1]:.2f}\")\n",
    "                ax2.set_yscale('log'); ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "\n",
    "        if (step + 1) % 1000 == 0 and HF_TOKEN and HF_TOKEN != \"hf_...\":\n",
    "            with open(LOSS_FILE, \"w\") as f:\n",
    "                for l in loss_history: f.write(f\"{l}\\n\")\n",
    "\n",
    "            torch.save(model.state_dict(), LIGHT_WEIGHTS_NAME)\n",
    "            full_checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'step': step,\n",
    "                'loss_history': loss_history,\n",
    "                'val_loss_history': val_loss_history,\n",
    "                'val_ppl_history': val_ppl_history,\n",
    "                'val_steps': val_steps\n",
    "            }\n",
    "            torch.save(full_checkpoint, FULL_CHECKPOINT_NAME)\n",
    "            try:\n",
    "                api.upload_file(path_or_fileobj=LIGHT_WEIGHTS_NAME, path_in_repo=f\"checkpoint-{step+1}/pytorch_model.bin\", repo_id=REPO_NAME)\n",
    "                api.upload_file(path_or_fileobj=FULL_CHECKPOINT_NAME, path_in_repo=\"latest_full_checkpoint.pth\", repo_id=REPO_NAME)\n",
    "                api.upload_file(path_or_fileobj=LOSS_FILE, path_in_repo=\"loss_history.txt\", repo_id=REPO_NAME)\n",
    "            except Exception as e: print(f\"‚ùå Upload Failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The old businessman was very angry when his computer\n",
      "Generating...\n",
      "\n",
      "Final Story:\n",
      "The old businessman was very angry when his computer was in the living room. She was very angry and she wanted to make the computer better. She asked her mom if she could help him. Her mom said yes, and she gave the computer a big hug.\n",
      "\n",
      "The computer was so happy! It was so big and bright. The computer was so big and bright. The computer was so big and bright. The computer was so big and bright.\n",
      "\n",
      "The computer was so happy to be alive. She hugged the computer and thanked it\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Load the tokenizer and your EXACT model architecture\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "config = GemmaZeroConfig()\n",
    "# Make sure to set num_key_value_heads to whatever you changed it to!\n",
    "model = GemmaZeroModel(config).to(\"cuda\")\n",
    "\n",
    "# 2. Download the saved weights\n",
    "from huggingface_hub import hf_hub_download\n",
    "weights_path = hf_hub_download(repo_id=\"FusionCorp/gemma-zero\", filename=\"checkpoint-10000/pytorch_model.bin\")\n",
    "\n",
    "# Load the raw dictionary\n",
    "state_dict = torch.load(weights_path, map_location=\"cuda\")\n",
    "\n",
    "# --- THE FIX: Remove the \"_orig_mod.\" prefix from the keys ---\n",
    "clean_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    clean_key = key.replace(\"_orig_mod.\", \"\")\n",
    "    clean_state_dict[clean_key] = value\n",
    "\n",
    "# Load the cleaned dictionary into your model\n",
    "model.load_state_dict(clean_state_dict)\n",
    "model.eval()\n",
    "\n",
    "# 3. Write a prompt and generate!\n",
    "prompt = \"The old businessman was very angry when his computer\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generating...\")\n",
    "\n",
    "with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    for _ in range(100): # Generate 50 words\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Pick the most likely next word\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "print(\"\\nFinal Story:\")\n",
    "print(tokenizer.decode(input_ids[0].cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation dataset...\n",
      "Calculating loss over 100 unseen batches...\n",
      "\n",
      "‚úÖ VALIDATION COMPLETE\n",
      "üìä Average Val Loss: 1.8160\n",
      "üìà Val Perplexity: 6.1470\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. SETUP & LOAD MODEL\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = GemmaZeroConfig() # Uses your class from the previous cells\n",
    "model = GemmaZeroModel(config).to(device)\n",
    "\n",
    "# 2. DOWNLOAD & CLEAN WEIGHTS (Handling the _orig_mod prefix)\n",
    "weights_path = hf_hub_download(repo_id=\"FusionCorp/gemma-zero\", filename=\"checkpoint-10000/pytorch_model.bin\")\n",
    "state_dict = torch.load(weights_path, map_location=device)\n",
    "clean_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(clean_dict)\n",
    "model.eval()\n",
    "\n",
    "# 3. PREPARE VALIDATION DATA (The \"Validation\" split)\n",
    "print(\"Loading validation dataset...\")\n",
    "val_ds = load_dataset(\"roneneldan/TinyStories\", split=\"validation\", streaming=True)\n",
    "\n",
    "def val_collate(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    tokens = tokenizer(texts, max_length=1024, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return tokens.input_ids.to(device)\n",
    "\n",
    "# We'll check 100 batches to get a very accurate average\n",
    "val_loader = DataLoader(val_ds, batch_size=8, collate_fn=val_collate)\n",
    "val_iter = iter(val_loader)\n",
    "\n",
    "# 4. RUN VALIDATION LOOP\n",
    "val_loss = 0\n",
    "num_batches = 100 \n",
    "\n",
    "print(f\"Calculating loss over {num_batches} unseen batches...\")\n",
    "with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    for i in range(num_batches):\n",
    "        inputs = next(val_iter)\n",
    "        labels = inputs.clone()\n",
    "        labels[labels == tokenizer.eos_token_id] = -100 # Ignore padding in loss\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        \n",
    "        # Flatten for CrossEntropy\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        loss = F.cross_entropy(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1), ignore_index=-100)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "avg_val_loss = val_loss / num_batches\n",
    "print(f\"\\n‚úÖ VALIDATION COMPLETE\")\n",
    "print(f\"üìä Average Val Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"üìà Val Perplexity: {torch.exp(torch.tensor(avg_val_loss)).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
