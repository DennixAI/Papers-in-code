{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: HF_TOKEN or REPO_NAME not found in .env file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2767391483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: HF_TOKEN or REPO_NAME not found in .env file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error: HF_TOKEN or REPO_NAME not found in .env file."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#load_dotenv()\n",
    "HF_TOKEN = \"\"\n",
    "REPO_NAME = \"\"\n",
    "\n",
    "if not HF_TOKEN or not REPO_NAME:\n",
    "    raise ValueError(\"Error: HF_TOKEN or REPO_NAME not found in .env file.\")\n",
    "\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "try:\n",
    "    create_repo(repo_id=REPO_NAME, repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Connected to Hugging Face Repo: {REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\" Repo Connection Failed: {e}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 50257      # Must be 50257 for GPT2\n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        k, v = k.repeat_interleave(self.num_key_value_groups, dim=1), v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if self.config.attn_logit_softcapping:\n",
    "            attn_weights = torch.tanh(attn_weights / self.config.attn_logit_softcapping) * self.config.attn_logit_softcapping\n",
    "        if attention_mask is not None: attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        attn_output = torch.matmul(F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype), v)\n",
    "        return self.o_proj(attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1))\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # This line saves ~10GB of VRAM by re-calculating \n",
    "                # activations during the backward pass.\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, \n",
    "                    x, \n",
    "                    None,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness_savant_dataloader(batch_size=4, seq_len=2048):\n",
    "    print(\"Loading datasets...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def get_col(row, candidates):\n",
    "        for col in candidates:\n",
    "            if col in row and row[col]: return str(row[col])\n",
    "        return \"\"\n",
    "\n",
    "    datasets_list = []\n",
    "    \n",
    "    try:\n",
    "        ds = load_dataset(\"onurSakar/GYM-Exercise\", split=\"train\")\n",
    "        def f(x): return {\"text\": f\"<|user|>\\nHow do I do {get_col(x, ['Title', 'title'])}?\\n<|model|>\\n{get_col(x, ['Desc', 'desc'])}<|endoftext|>\"}\n",
    "        datasets_list.append(ds.map(f, remove_columns=ds.column_names))\n",
    "    except: print(\"Skipping Gym\")\n",
    "\n",
    "    try:\n",
    "        ds = load_dataset(\"issai/LLM_for_Dietary_Recommendation_System\", split=\"train\")\n",
    "        def f(x): return {\"text\": f\"<|user|>\\nDiet plan for:\\n{get_col(x, ['Profile', 'input'])}\\n<|model|>\\n{get_col(x, ['Recommendation', 'output'])}<|endoftext|>\"}\n",
    "        datasets_list.append(ds.map(f, remove_columns=ds.column_names))\n",
    "    except: print(\"Skipping Diet\")\n",
    "\n",
    "    try:\n",
    "        ds = load_dataset(\"chibbss/fitness-chat-prompt-completion-dataset\", split=\"train\")\n",
    "        def f(x): return {\"text\": f\"<|user|>\\n{get_col(x, ['instruction'])}\\n<|model|>\\n{get_col(x, ['output'])}<|endoftext|>\"}\n",
    "        datasets_list.append(ds.map(f, remove_columns=ds.column_names))\n",
    "    except: print(\"Skipping Chat\")\n",
    "\n",
    "    dataset = concatenate_datasets(datasets_list).shuffle(seed=42)\n",
    "    \n",
    "    def data_generator():\n",
    "        while True:\n",
    "            for item in dataset:\n",
    "                yield torch.tensor(tokenizer.encode(item['text'], max_length=seq_len, truncation=True, padding=\"max_length\"))\n",
    "    \n",
    "    gen = data_generator()\n",
    "    return lambda: torch.stack([next(gen) for _ in range(batch_size)]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hLoading datasets...\n",
      " Training 400M Model | Brain Density: 24 Layers\n",
      "Effective Batch: 32 | Optimizer: 8-bit AdamW\n",
      "Step 32 | Loss: 7.8556 | Peak VRAM: 11.68 GB\n",
      "Step 64 | Loss: 5.7824 | Peak VRAM: 11.68 GB\n",
      "Step 96 | Loss: 1.7703 | Peak VRAM: 11.68 GB\n",
      "Step 128 | Loss: 0.1469 | Peak VRAM: 11.68 GB\n",
      "Step 160 | Loss: 0.1252 | Peak VRAM: 11.68 GB\n",
      "Step 192 | Loss: 0.1168 | Peak VRAM: 11.68 GB\n",
      "Step 224 | Loss: 0.0976 | Peak VRAM: 11.68 GB\n",
      "Step 256 | Loss: 0.0876 | Peak VRAM: 11.68 GB\n",
      "Step 288 | Loss: 0.0699 | Peak VRAM: 11.68 GB\n",
      "Step 320 | Loss: 0.9397 | Peak VRAM: 11.68 GB\n",
      "Step 352 | Loss: 0.0532 | Peak VRAM: 11.68 GB\n",
      "Step 384 | Loss: 0.0502 | Peak VRAM: 11.68 GB\n",
      "Step 416 | Loss: 0.0468 | Peak VRAM: 11.68 GB\n",
      "Step 448 | Loss: 0.0434 | Peak VRAM: 11.68 GB\n",
      "Step 480 | Loss: 0.0398 | Peak VRAM: 11.68 GB\n",
      "Step 512 | Loss: 0.0362 | Peak VRAM: 11.68 GB\n",
      "Step 544 | Loss: 0.0317 | Peak VRAM: 11.68 GB\n",
      "Step 576 | Loss: 0.0277 | Peak VRAM: 11.68 GB\n",
      "Step 608 | Loss: 0.0239 | Peak VRAM: 11.68 GB\n",
      "Step 640 | Loss: 0.0207 | Peak VRAM: 11.68 GB\n",
      "Step 672 | Loss: 0.3412 | Peak VRAM: 11.68 GB\n",
      "Step 704 | Loss: 0.0177 | Peak VRAM: 11.68 GB\n",
      "Step 736 | Loss: 0.0151 | Peak VRAM: 11.68 GB\n",
      "Step 768 | Loss: 0.0156 | Peak VRAM: 11.68 GB\n",
      "Step 800 | Loss: 0.0134 | Peak VRAM: 11.68 GB\n",
      "Step 832 | Loss: 0.0143 | Peak VRAM: 11.68 GB\n",
      "Step 864 | Loss: 0.0116 | Peak VRAM: 11.68 GB\n",
      "Step 896 | Loss: 0.0116 | Peak VRAM: 11.68 GB\n",
      "Step 928 | Loss: 0.0122 | Peak VRAM: 11.68 GB\n",
      "Step 960 | Loss: 0.0120 | Peak VRAM: 11.68 GB\n",
      "Step 992 | Loss: 0.0115 | Peak VRAM: 11.68 GB\n",
      "Step 1024 | Loss: 0.0110 | Peak VRAM: 11.68 GB\n",
      "Step 1056 | Loss: 0.0107 | Peak VRAM: 11.68 GB\n",
      "Step 1088 | Loss: 0.0177 | Peak VRAM: 11.68 GB\n",
      "Step 1120 | Loss: 0.0102 | Peak VRAM: 11.68 GB\n",
      "Step 1152 | Loss: 0.0100 | Peak VRAM: 11.68 GB\n",
      "Step 1184 | Loss: 0.0109 | Peak VRAM: 11.68 GB\n",
      "Step 1216 | Loss: 0.0108 | Peak VRAM: 11.68 GB\n",
      "Step 1248 | Loss: 0.0099 | Peak VRAM: 11.68 GB\n",
      "Step 1280 | Loss: 0.0103 | Peak VRAM: 11.68 GB\n",
      "Step 1312 | Loss: 0.0115 | Peak VRAM: 11.68 GB\n",
      "Step 1344 | Loss: 0.0113 | Peak VRAM: 11.68 GB\n",
      "Step 1376 | Loss: 0.0110 | Peak VRAM: 11.68 GB\n",
      "Step 1408 | Loss: 0.0108 | Peak VRAM: 11.68 GB\n",
      "Step 1440 | Loss: 0.0098 | Peak VRAM: 11.68 GB\n",
      "Step 1472 | Loss: 0.0097 | Peak VRAM: 11.68 GB\n",
      "Step 1504 | Loss: 0.0097 | Peak VRAM: 11.68 GB\n",
      "Step 1536 | Loss: 0.0096 | Peak VRAM: 11.68 GB\n",
      "Step 1568 | Loss: 0.0096 | Peak VRAM: 11.68 GB\n",
      "Step 1600 | Loss: 0.0104 | Peak VRAM: 11.68 GB\n",
      "Step 1632 | Loss: 0.0096 | Peak VRAM: 11.68 GB\n",
      "Step 1664 | Loss: 0.0106 | Peak VRAM: 11.68 GB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mInvalid response: 404 Not Found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q bitsandbytes \n",
    "\n",
    "import bitsandbytes as bnb # Import the 8-bit optimizer\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\"\n",
    "    config = GemmaZeroConfig() \n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable() \n",
    "\n",
    "    # 8-Bit Optimizer (Saves 2.4GB VRAM)\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "    scaler = GradScaler(device='cuda')\n",
    "\n",
    "    # Hyperparameters for 16GB\n",
    "    BATCH_SIZE = 2      # Lowered to 2 to prevent OOM\n",
    "    ACCUM_STEPS = 16    # Effective Batch Size = 32 (2 * 16)\n",
    "    SEQ_LEN = 2048\n",
    "    TOTAL_STEPS = 5000\n",
    "    \n",
    "    get_batch = get_fitness_savant_dataloader(BATCH_SIZE, SEQ_LEN)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True) # Saves extra memory\n",
    "    \n",
    "    print(f\" Training 400M Model | Brain Density: {config.num_hidden_layers} Layers\")\n",
    "    print(f\"Effective Batch: {BATCH_SIZE * ACCUM_STEPS} | Optimizer: 8-bit AdamW\")\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "        inputs = get_batch()\n",
    "        \n",
    "        # Safety Check\n",
    "        if inputs.max() >= config.vocab_size:\n",
    "            print(f\"Error: Token {inputs.max()} exceeds vocab {config.vocab_size}\")\n",
    "            break\n",
    "            \n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16): \n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Causal LM Shift\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Cross Entropy Loss\n",
    "            loss = F.cross_entropy(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
    "            loss = loss / ACCUM_STEPS \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Monitoring\n",
    "            if (step + 1) % (ACCUM_STEPS * 2) == 0:\n",
    "                mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "                print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f} | Peak VRAM: {mem:.2f} GB\")\n",
    "        \n",
    "        if (step + 1) % 200 == 0:\n",
    "                print(f\"☁️ Uploading Checkpoint to {REPO_NAME}...\")\n",
    "                \n",
    "                # 1. Save weights locally first\n",
    "                local_file = \"pytorch_model.bin\"\n",
    "                torch.save(model.state_dict(), local_file)\n",
    "                \n",
    "                # 2. Push to Hugging Face\n",
    "                try:\n",
    "                    api.upload_file(\n",
    "                        path_or_fileobj=local_file,\n",
    "                        path_in_repo=\"pytorch_model.bin\", # Overwrites the file in the repo\n",
    "                        repo_id=REPO_NAME,\n",
    "                        repo_type=\"model\"\n",
    "                    )\n",
    "                    print(f\"Upload Success at Step {step+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Upload Failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
