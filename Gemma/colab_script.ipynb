{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Hugging Face Repo: FusionCorp/gemma-zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    print(\"Installing dependencies...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"-q\", \"datasets\", \"transformers\", \"accelerate\"])\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "else:\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "\n",
    "if not HF_TOKEN or not REPO_NAME:\n",
    "    raise ValueError(\"Error: HF_TOKEN or REPO_NAME not found in .env file.\")\n",
    "\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "try:\n",
    "    create_repo(repo_id=REPO_NAME, repo_type=\"model\", token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Connected to Hugging Face Repo: {REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\" Repo Connection Failed: {e}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GemmaZeroConfig:\n",
    "    vocab_size: int = 50257      # Must be 50257 for GPT2\n",
    "    hidden_size: int = 1024      \n",
    "    intermediate_size: int = 4096 \n",
    "    num_hidden_layers: int = 24  \n",
    "    num_attention_heads: int = 16 \n",
    "    num_key_value_heads: int = 4 \n",
    "    head_dim: int = 64\n",
    "    max_position_embeddings: int = 2048 \n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attn_logit_softcapping: float = 50.0\n",
    "    final_logit_softcapping: float = 30.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaRMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        x_float = x.float()\n",
    "        variance = x_float.pow(2).mean(-1, keepdim=True)\n",
    "        x_float = x_float * torch.rsqrt(variance + self.eps)\n",
    "        return (x_float * self.weight.float()).type_as(x) + 1.0\n",
    "\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, x, seq_len=None):\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    def rotate_half(x): return torch.cat((-x[..., x.shape[-1] // 2:], x[..., :x.shape[-1] // 2]), dim=-1)\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class GemmaAttention(nn.Module):\n",
    "    def __init__(self, config: GemmaZeroConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        self.rotary_emb = GemmaRotaryEmbedding(self.head_dim, config.max_position_embeddings, config.rope_theta)\n",
    "\n",
    "        # GEMMA 3: QK-Norm (RMSNorm on Queries and Keys)\n",
    "        # This stabilizes training and allows us to use Flash Attention\n",
    "        self.q_norm = GemmaRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
    "        self.k_norm = GemmaRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        \n",
    "        # 1. Projections\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        # 2. QK Norm (Gemma 3 feature)\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        # 3. RoPE\n",
    "        # Transpose for RoPE: (bsz, heads, seq, dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(v, seq_len=q_len)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        # 4. GQA Expansion\n",
    "        # (Expand K and V to match Q heads for calculation)\n",
    "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "\n",
    "        # 5. Flash Attention (Much faster, less memory)\n",
    "        # We drop manual soft-capping here to enable Flash Attention. \n",
    "        # QK-Norm handles the stability role of soft-capping.\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v, \n",
    "            attn_mask=None, # Flash attn handles causal mask internally if is_causal=True\n",
    "            dropout_p=0.0, \n",
    "            is_causal=True\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class GemmaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = GemmaAttention(config)\n",
    "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.hidden_size, config.intermediate_size, bias=False), \n",
    "            nn.Linear(config.intermediate_size, config.hidden_size, bias=False) \n",
    "        )\n",
    "        self.mlp_gate = self.mlp[0]; self.mlp_up = self.mlp[1]; self.mlp_down = self.mlp[2]\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        r = x; x = self.input_layernorm(x); x = self.self_attn(x, attention_mask=mask); x = r + x\n",
    "        r = x; x = self.post_attention_layernorm(x)\n",
    "        gate, val = self.mlp_gate(x), self.mlp_up(x)\n",
    "        x = self.mlp_down(F.gelu(gate) * val)\n",
    "        return r + x\n",
    "\n",
    "class GemmaZeroModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([GemmaBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.embed_scale = math.sqrt(config.hidden_size)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def gradient_checkpointing_enable(self): self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        for layer in self.layers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # This line saves ~10GB of VRAM by re-calculating \n",
    "                # activations during the backward pass.\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, \n",
    "                    x, \n",
    "                    None,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = torch.matmul(x, self.embed_tokens.weight.t())\n",
    "        if self.config.final_logit_softcapping:\n",
    "             logits = torch.tanh(logits / self.config.final_logit_softcapping) * self.config.final_logit_softcapping\n",
    "             \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class TinyStoriesDataset(IterableDataset):\n",
    "    def __init__(self, seq_len=2048):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        for item in iterator:\n",
    "            if len(item['text']) < 50: continue\n",
    "            \n",
    "            # Simple truncation/padding\n",
    "            tokens = self.tokenizer(\n",
    "                item['text'], \n",
    "                max_length=self.seq_len, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            yield tokens.input_ids.squeeze(0)\n",
    "\n",
    "def get_tinystories_loader(batch_size=4, seq_len=2048):\n",
    "    ds = TinyStoriesDataset(seq_len=seq_len)\n",
    "    # num_workers=2 runs tokenization in parallel background processes\n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff45a5bd0248b2897a246af61b3692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training 400M Model | Brain Density: 24 Layers\n",
      "Effective Batch: 32 | Optimizer: 8-bit AdamW\n",
      "Step 32 | Loss: 26.7978 | Peak VRAM: 8.04 GB\n",
      "Step 64 | Loss: 21.8812 | Peak VRAM: 8.04 GB\n",
      "Step 96 | Loss: 19.9212 | Peak VRAM: 8.04 GB\n",
      "Step 128 | Loss: 17.2931 | Peak VRAM: 8.04 GB\n",
      "Step 160 | Loss: 14.2393 | Peak VRAM: 8.04 GB\n",
      "Step 192 | Loss: 16.1883 | Peak VRAM: 8.04 GB\n",
      "‚òÅÔ∏è Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2799ba5fe34d497198672b950e365812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c5a9e7ae6849dd95ce4960c719c3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5db4c41176948468f452c101bec8efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 200\n",
      "Step 224 | Loss: 14.0803 | Peak VRAM: 8.04 GB\n",
      "Step 256 | Loss: 14.3976 | Peak VRAM: 8.04 GB\n",
      "Step 288 | Loss: 13.5689 | Peak VRAM: 8.04 GB\n",
      "Step 320 | Loss: 12.3272 | Peak VRAM: 8.04 GB\n",
      "Step 352 | Loss: 15.1846 | Peak VRAM: 8.04 GB\n",
      "Step 384 | Loss: 15.2465 | Peak VRAM: 8.04 GB\n",
      "‚òÅÔ∏è Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e98ddc05a324eea8dee00370d0d7910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90307535c29d4c43a7edc0a4e61e90e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d308646b5f4e8898b2f93ac6f65d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 400\n",
      "Step 416 | Loss: 13.2133 | Peak VRAM: 8.04 GB\n",
      "Step 448 | Loss: 11.7567 | Peak VRAM: 8.04 GB\n",
      "Step 480 | Loss: 13.4908 | Peak VRAM: 8.04 GB\n",
      "Step 512 | Loss: 12.5683 | Peak VRAM: 8.04 GB\n",
      "Step 544 | Loss: 12.0133 | Peak VRAM: 8.04 GB\n",
      "Step 576 | Loss: 14.6466 | Peak VRAM: 8.04 GB\n",
      "‚òÅÔ∏è Uploading Checkpoint to FusionCorp/gemma-zero...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17608d4d77d44bb833b7ec39075ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd121f340a7427c9ce385be8c1e23dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caeb72d13a24925b5424010a6f236dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  pytorch_model.bin           :   0%|          |  557kB / 1.67GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Success at Step 600\n",
      "Step 608 | Loss: 12.0771 | Peak VRAM: 8.04 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1682137808.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1682137808.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mACCUM_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q bitsandbytes as bnb\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup # Add this import\n",
    "\n",
    "def train():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    config = GemmaZeroConfig()\n",
    "    model = GemmaZeroModel(config).to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Optional Speedup\n",
    "    try: model = torch.compile(model)\n",
    "    except: pass\n",
    "\n",
    "    # Optimizer \n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Hyperparams (Adjusted for Flash Attention efficiency)\n",
    "    BATCH_SIZE = 8       # Was 2 , bigger because of flash attention\n",
    "    ACCUM_STEPS = 4      # Was 16 (Total effective batch still 32)  \n",
    "    SEQ_LEN = 1024       \n",
    "    TOTAL_STEPS = 5000\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 100, TOTAL_STEPS)\n",
    "    dataloader = DataLoader(TinyStoriesDataset(SEQ_LEN), batch_size=BATCH_SIZE, num_workers=0)\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    pad_token_id = 50256\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    print(f\"üöÄ Training 400M Model | Stability Mode: ON\")\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "        # Optimized Data Fetching\n",
    "        try: inputs = next(data_iter).to(device)\n",
    "        except StopIteration: data_iter = iter(dataloader); inputs = next(data_iter).to(device)\n",
    "        \n",
    "        labels = inputs.clone()\n",
    "        labels[labels == pad_token_id] = -100\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(\n",
    "                logits[..., :-1, :].contiguous().view(-1, config.vocab_size),\n",
    "                labels[..., 1:].contiguous().view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            loss = loss / ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if (step + 1) % 50 == 0:\n",
    "                 print(f\"Step {step+1} | Loss: {loss.item() * ACCUM_STEPS:.4f}\")\n",
    "\n",
    "        # Upload Logic\n",
    "        if (step + 1) % 200 == 0 and HF_TOKEN and HF_TOKEN != \"hf_...\":\n",
    "            print(f\"‚òÅÔ∏è Uploading to Gemma-zero...\")\n",
    "            torch.save(model.state_dict(), \"pytorch_model.bin\")\n",
    "            try:\n",
    "                api.upload_file(path_or_fileobj=\"pytorch_model.bin\", path_in_repo=f\"checkpoint-{step+1}/pytorch_model.bin\", repo_id=REPO_NAME, repo_type=\"model\")\n",
    "                print(\"‚úÖ Upload Success!\")\n",
    "            except Exception as e: print(f\"‚ùå Upload Failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df515c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7864a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395db0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
